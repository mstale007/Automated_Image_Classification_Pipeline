{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COVID-XRAYS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP0UfR6hh2gS9naF4bZDY1K",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mstale007/Automated_Image_Classification_Pipeline/blob/master/COVID_XRAYS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqLA_uFFYN7W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "25ddf0b2-dad7-4940-a148-b076c7b740ff"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import keras\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.layers import Dense,GlobalAveragePooling2D,Flatten, Dropout\n",
        "from keras.applications import MobileNet\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.mobilenet import preprocess_input\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Model\n",
        "from keras.optimizers import Adam"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_lzV9ErQYdeU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "ec52c91f-5d13-4949-b5ff-801251dc0eab"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YahTtbmpgMWc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 849
        },
        "outputId": "a4487355-5e61-442e-b03f-96a6803ad682"
      },
      "source": [
        "import keras.applications as ka\n",
        "dir(ka)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['DenseNet121',\n",
              " 'DenseNet169',\n",
              " 'DenseNet201',\n",
              " 'InceptionResNetV2',\n",
              " 'InceptionV3',\n",
              " 'MobileNet',\n",
              " 'MobileNetV2',\n",
              " 'NASNetLarge',\n",
              " 'NASNetMobile',\n",
              " 'ResNet101',\n",
              " 'ResNet101V2',\n",
              " 'ResNet152',\n",
              " 'ResNet152V2',\n",
              " 'ResNet50',\n",
              " 'ResNet50V2',\n",
              " 'VGG16',\n",
              " 'VGG19',\n",
              " 'Xception',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " 'absolute_import',\n",
              " 'backend',\n",
              " 'densenet',\n",
              " 'division',\n",
              " 'inception_resnet_v2',\n",
              " 'inception_v3',\n",
              " 'keras_applications',\n",
              " 'keras_modules_injection',\n",
              " 'layers',\n",
              " 'mobilenet',\n",
              " 'mobilenet_v2',\n",
              " 'models',\n",
              " 'nasnet',\n",
              " 'print_function',\n",
              " 'resnet',\n",
              " 'resnet50',\n",
              " 'resnet_v2',\n",
              " 'utils',\n",
              " 'vgg16',\n",
              " 'vgg19',\n",
              " 'xception']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRVz8RKSgou7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications import DenseNet201\n",
        "from keras.applications import InceptionResNetV2\n",
        "from keras.applications import InceptionV3\n",
        "from keras.applications import MobileNetV2\n",
        "from keras.applications import NASNetLarge\n",
        "from keras.applications import NASNetMobile\n",
        "from keras.applications import ResNet101\n",
        "from keras.applications import ResNet152V2\n",
        "from keras.applications import VGG16\n",
        "from keras.applications import VGG19\n",
        "from keras.applications import Xception"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GA148ekrnuZ9",
        "colab_type": "text"
      },
      "source": [
        "# MobileNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAI7czCfYaD7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "4be68600-32f4-4d89-8025-30ec4efcf9a9"
      },
      "source": [
        "base_model=MobileNet(weights='imagenet',include_top=False) #imports the mobilenet model and discards the last 1000 neuron layer."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_applications/mobilenet.py:207: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  warnings.warn('`input_shape` is undefined or non-square, '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_224_tf_no_top.h5\n",
            "17227776/17225924 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDZtBNLqcTiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x1=base_model.output\n",
        "x1=GlobalAveragePooling2D()(x1)\n",
        "#x1=Flatten(name=\"flatten\")(x1)\n",
        "x1=Dense(64,activation='relu')(x1) #we add dense layers so that the model can learn more complex1 functions and classify for better results.\n",
        "x1=Dropout(0.5)(x1)\n",
        "#x1=Dense(1024,activation='relu')(x1) #dense layer 2\n",
        "#x1=Dense(512,activation='relu')(x1) #dense layer 3\n",
        "preds=Dense(2,activation='softmax')(x1) #final layer with softmax activation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSMw9oCTc5PQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model=Model(inputs=base_model.input,outputs=preds)\n",
        "#specify the inputs\n",
        "#specify the outputs\n",
        "#now a model has been created based on our architecture"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHLJkRKJdZk3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in model.layers[:20]:\n",
        "    layer.trainable=False\n",
        "for layer in model.layers[20:]:\n",
        "    layer.trainable=True\n",
        "# for layer in base_model.layers[:]:\n",
        "#     layer.trainable=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtEsOhmfdgZY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1d6bf684-12bd-41e6-aa44-d3178a5e7fdc"
      },
      "source": [
        "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
        "\n",
        "train_generator=train_datagen.flow_from_directory('/gdrive/My Drive/COVID Dataset/data/train/', # this is where you specify the path to the main data folder\n",
        "                                                 target_size=(224,224),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 8624 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zy76wizbfy_G",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "9d34c9e5-4fa4-4f0b-ab79-3c0117dade34"
      },
      "source": [
        "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "EPOCHS=10\n",
        "# Adam optimizer\n",
        "# loss function will be categorical cross entropy\n",
        "# evaluation metric will be accuracy\n",
        "\n",
        "step_size_train=train_generator.n//train_generator.batch_size\n",
        "history=model.fit_generator(generator=train_generator,\n",
        "                   steps_per_epoch=step_size_train,\n",
        "                   epochs=EPOCHS,verbose=1)\n",
        "print(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "269/269 [==============================] - 4064s 15s/step - loss: 0.0824 - accuracy: 0.9766\n",
            "Epoch 2/10\n",
            "269/269 [==============================] - 184s 685ms/step - loss: 0.0409 - accuracy: 0.9861\n",
            "Epoch 3/10\n",
            "269/269 [==============================] - 162s 601ms/step - loss: 0.0225 - accuracy: 0.9930\n",
            "Epoch 4/10\n",
            "269/269 [==============================] - 160s 596ms/step - loss: 0.0177 - accuracy: 0.9946\n",
            "Epoch 5/10\n",
            "269/269 [==============================] - 160s 596ms/step - loss: 0.0145 - accuracy: 0.9953\n",
            "Epoch 6/10\n",
            "269/269 [==============================] - 160s 596ms/step - loss: 0.0154 - accuracy: 0.9943\n",
            "Epoch 7/10\n",
            "269/269 [==============================] - 160s 595ms/step - loss: 0.0054 - accuracy: 0.9986\n",
            "Epoch 8/10\n",
            "269/269 [==============================] - 159s 592ms/step - loss: 0.0094 - accuracy: 0.9980\n",
            "Epoch 9/10\n",
            "269/269 [==============================] - 157s 585ms/step - loss: 0.0155 - accuracy: 0.9944\n",
            "Epoch 10/10\n",
            "269/269 [==============================] - 160s 596ms/step - loss: 0.0055 - accuracy: 0.9985\n",
            "<keras.callbacks.callbacks.History object at 0x7fb0397a1ac8>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrN7APESHe_S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8af787f9-c3a2-4e55-a6ca-4a67e232cfc7"
      },
      "source": [
        "test_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
        "\n",
        "test_generator=train_datagen.flow_from_directory('/gdrive/My Drive/COVID Dataset/data/test/', # this is where you specify the path to the main data folder\n",
        "                                                 target_size=(224,224),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 shuffle=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1005 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVEkLShGlJHD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "83b33eb4-96a2-459d-c7e4-561c38df6a19"
      },
      "source": [
        "scores=model.evaluate_generator(generator=test_generator,verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 439s 14s/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cI_wfJJeXF2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "eda38f3d-7527-4a03-90d0-bdef66133fbe"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "Y_pred = model.predict_generator(test_generator)\n",
        "y_pred_MobileNet = np.argmax(Y_pred,axis=1)\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(test_generator.classes, y_pred_MobileNet))\n",
        "print('Classification Report')\n",
        "target_names = ['Normal', 'Positive']\n",
        "print(classification_report(test_generator.classes, y_pred_MobileNet, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Confusion Matrix\n",
            "[[  1  99]\n",
            " [ 40 865]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.02      0.01      0.01       100\n",
            "    Positive       0.90      0.96      0.93       905\n",
            "\n",
            "    accuracy                           0.86      1005\n",
            "   macro avg       0.46      0.48      0.47      1005\n",
            "weighted avg       0.81      0.86      0.83      1005\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0wJLEWHcXk3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4f7974f-00e2-4b9c-e122-057cb028c457"
      },
      "source": [
        "print(history.history.keys())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['loss', 'accuracy'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQ3TXFUjaOhk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f651e740-86bf-4996-d158-3b9128a38471"
      },
      "source": [
        "N = EPOCHS\n",
        "plt.style.use(\"ggplot\")\n",
        "plt.figure()\n",
        "plt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\n",
        "#plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n",
        "plt.plot(np.arange(0, N), history.history[\"accuracy\"], label=\"train_acc\")\n",
        "#plt.plot(np.arange(0, N), history.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "plt.title(\"COVID-19 Dataset\")\n",
        "plt.xlabel(\"Epoch #\")\n",
        "plt.ylabel(\"Loss/Accuracy\")\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.savefig(\"MobileNet.png\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fxt-xtEUREww",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c0d0e276-e020-4b0b-b2a2-446becde84d5"
      },
      "source": [
        "print(\"Accuracy = \", scores[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy =  0.937313437461853\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWf8YO9qT6-G",
        "colab_type": "text"
      },
      "source": [
        "# DenseNet201"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CSuWai14X8j_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "outputId": "2e07f5ab-d927-4410-e2d5-7c5f6f87b10f"
      },
      "source": [
        "base_model=DenseNet201(weights='imagenet',include_top=False) #imports the mobilenet model and discards the last 1000 neuron layer.\n",
        "x1=base_model.output\n",
        "x1=GlobalAveragePooling2D(pool_size=(4,4))(x1)\n",
        "x1=Flatten(name=\"flatten\")(x1)\n",
        "x1=Dense(64,activation='relu')(x1) #we add dense layers so that the model can learn more complex1 functions and classify for better results.\n",
        "x1=Dropout(0.5)(x1)\n",
        "#x1=Dense(1024,activation='relu')(x1) #dense layer 2\n",
        "#x1=Dense(512,activation='relu')(x1) #dense layer 3\n",
        "preds=Dense(2,activation='softmax')(x1) #final layer with softmax activation\n",
        "\n",
        "# x1=base_model.output\n",
        "# x1=GlobalAveragePooling2D()(x1)\n",
        "# x1=Dense(1024,activation='relu')(x1) #we add dense layers so that the model can learn more complex1 functions and classify for better results.\n",
        "# x1=Dense(1024,activation='relu')(x1) #dense layer 2\n",
        "# x1=Dense(512,activation='relu')(x1) #dense layer 3\n",
        "# preds=Dense(2,activation='softmax')(x1) #final layer with softmax activation\n",
        "\n",
        "model=Model(inputs=base_model.input,outputs=preds)\n",
        "#specify the inputs\n",
        "#specify the outputs\n",
        "#now a model has been created based on our architecture\n",
        "\n",
        "for layer in model.layers[:20]:\n",
        "    layer.trainable=False\n",
        "for layer in model.layers[20:]:\n",
        "    layer.trainable=True\n",
        "\n",
        "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
        "\n",
        "train_generator=train_datagen.flow_from_directory('/gdrive/My Drive/COVID Dataset/Train/', # this is where you specify the path to the main data folder\n",
        "                                                 target_size=(224,224),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 shuffle=True)\n",
        "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "# Adam optimizer\n",
        "# loss function will be categorical cross entropy\n",
        "# evaluation metric will be accuracy\n",
        "\n",
        "step_size_train=train_generator.n//train_generator.batch_size\n",
        "history=model.fit_generator(generator=train_generator,\n",
        "                   steps_per_epoch=step_size_train,\n",
        "                   epochs=10,verbose=1)\n",
        "test_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
        "\n",
        "test_generator=train_datagen.flow_from_directory('/gdrive/My Drive/COVID Dataset/Test/', # this is where you specify the path to the main data folder\n",
        "                                                 target_size=(224,224),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 shuffle=True)\n",
        "scores=model.evaluate_generator(generator=test_generator,verbose=1)\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "Y_pred = model.predict_generator(test_generator)\n",
        "y_pred_DenseNet201 = np.argmax(Y_pred,axis=1)\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(test_generator.classes, y_pred_DenseNet201))\n",
        "print('Classification Report')\n",
        "target_names = ['Normal', 'Positive']\n",
        "print(classification_report(test_generator.classes, y_pred_DenseNet201, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/keras-team/keras-applications/releases/download/densenet/densenet201_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "74842112/74836368 [==============================] - 7s 0us/step\n",
            "Found 348 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "10/10 [==============================] - 122s 12s/step - loss: 0.5536 - accuracy: 0.8250\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 30s 3s/step - loss: 0.0187 - accuracy: 0.9936\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 23s 2s/step - loss: 4.7462e-04 - accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 23s 2s/step - loss: 0.0302 - accuracy: 0.9906\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 23s 2s/step - loss: 0.0857 - accuracy: 0.9872\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 23s 2s/step - loss: 0.0066 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 23s 2s/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 23s 2s/step - loss: 3.0352e-04 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.6279e-04 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 23s 2s/step - loss: 1.0099e-04 - accuracy: 1.0000\n",
            "Found 59 images belonging to 2 classes.\n",
            "2/2 [==============================] - 12s 6s/step\n",
            "Confusion Matrix\n",
            "[[10  7]\n",
            " [15 27]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.40      0.59      0.48        17\n",
            "    Positive       0.79      0.64      0.71        42\n",
            "\n",
            "    accuracy                           0.63        59\n",
            "   macro avg       0.60      0.62      0.59        59\n",
            "weighted avg       0.68      0.63      0.64        59\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3teQvEjT_rc",
        "colab_type": "text"
      },
      "source": [
        "# InceptionResNetV2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfVVELSQSuuN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "outputId": "42e5e13c-8542-499e-d5b3-d8eaba95ff7b"
      },
      "source": [
        "base_model=InceptionResNetV2(weights='imagenet',include_top=False) #imports the mobilenet model and discards the last 1000 neuron layer.\n",
        "x1=base_model.output\n",
        "x1=GlobalAveragePooling2D(pool_size=(4,4))(x1)\n",
        "x1=Flatten(name=\"flatten\")(x1)\n",
        "x1=Dense(64,activation='relu')(x1) #we add dense layers so that the model can learn more complex1 functions and classify for better results.\n",
        "x1=Dropout(0.5)(x1)\n",
        "#x1=Dense(1024,activation='relu')(x1) #dense layer 2\n",
        "#x1=Dense(512,activation='relu')(x1) #dense layer 3\n",
        "preds=Dense(2,activation='softmax')(x1) #final layer with softmax activation\n",
        "\n",
        "# x1=base_model.output\n",
        "# x1=GlobalAveragePooling2D()(x1)\n",
        "# x1=Dense(1024,activation='relu')(x1) #we add dense layers so that the model can learn more complex1 functions and classify for better results.\n",
        "# x1=Dense(1024,activation='relu')(x1) #dense layer 2\n",
        "# x1=Dense(512,activation='relu')(x1) #dense layer 3\n",
        "# preds=Dense(2,activation='softmax')(x1) #final layer with softmax activation\n",
        "\n",
        "model=Model(inputs=base_model.input,outputs=preds)\n",
        "#specify the inputs\n",
        "#specify the outputs\n",
        "#now a model has been created based on our architecture\n",
        "\n",
        "for layer in model.layers[:20]:\n",
        "    layer.trainable=False\n",
        "for layer in model.layers[20:]:\n",
        "    layer.trainable=True\n",
        "\n",
        "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
        "\n",
        "train_generator=train_datagen.flow_from_directory('/gdrive/My Drive/COVID Dataset/Train/', # this is where you specify the path to the main data folder\n",
        "                                                 target_size=(224,224),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 shuffle=True)\n",
        "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "# Adam optimizer\n",
        "# loss function will be categorical cross entropy\n",
        "# evaluation metric will be accuracy\n",
        "\n",
        "step_size_train=train_generator.n//train_generator.batch_size\n",
        "history=model.fit_generator(generator=train_generator,\n",
        "                   steps_per_epoch=step_size_train,\n",
        "                   epochs=10,verbose=1)\n",
        "test_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
        "\n",
        "test_generator=train_datagen.flow_from_directory('/gdrive/My Drive/COVID Dataset/Test/', # this is where you specify the path to the main data folder\n",
        "                                                 target_size=(224,224),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 shuffle=True)\n",
        "scores=model.evaluate_generator(generator=test_generator,verbose=1)\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "Y_pred = model.predict_generator(test_generator)\n",
        "y_pred_InceptionResNetV2 = np.argmax(Y_pred,axis=1)\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(test_generator.classes, y_pred_InceptionResNetV2))\n",
        "print('Classification Report')\n",
        "target_names = ['Normal', 'Positive']\n",
        "print(classification_report(test_generator.classes, y_pred_InceptionResNetV2, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.7/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "219062272/219055592 [==============================] - 18s 0us/step\n",
            "Found 348 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "10/10 [==============================] - 98s 10s/step - loss: 0.3566 - accuracy: 0.8703\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 18s 2s/step - loss: 0.0275 - accuracy: 0.9873\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 18s 2s/step - loss: 0.0147 - accuracy: 0.9937\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 18s 2s/step - loss: 0.0458 - accuracy: 0.9840\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 18s 2s/step - loss: 0.0056 - accuracy: 1.0000\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 17s 2s/step - loss: 9.4629e-05 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 18s 2s/step - loss: 0.0014 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 18s 2s/step - loss: 0.0092 - accuracy: 0.9936\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 18s 2s/step - loss: 0.0056 - accuracy: 0.9969\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 18s 2s/step - loss: 0.0153 - accuracy: 0.9937\n",
            "Found 59 images belonging to 2 classes.\n",
            "2/2 [==============================] - 10s 5s/step\n",
            "Confusion Matrix\n",
            "[[ 4 13]\n",
            " [13 29]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.24      0.24      0.24        17\n",
            "    Positive       0.69      0.69      0.69        42\n",
            "\n",
            "    accuracy                           0.56        59\n",
            "   macro avg       0.46      0.46      0.46        59\n",
            "weighted avg       0.56      0.56      0.56        59\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxiJ54dpUEoK",
        "colab_type": "text"
      },
      "source": [
        "# InceptionV3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaBBD3ufSlYS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "outputId": "02a79979-6e4e-4c4a-c09a-5576e4782dc7"
      },
      "source": [
        "base_model=InceptionV3(weights='imagenet',include_top=False) #imports the mobilenet model and discards the last 1000 neuron layer.\n",
        "x1=base_model.output\n",
        "x1=GlobalAveragePooling2D(pool_size=(4,4))(x1)\n",
        "x1=Flatten(name=\"flatten\")(x1)\n",
        "x1=Dense(64,activation='relu')(x1) #we add dense layers so that the model can learn more complex1 functions and classify for better results.\n",
        "x1=Dropout(0.5)(x1)\n",
        "#x1=Dense(1024,activation='relu')(x1) #dense layer 2\n",
        "#x1=Dense(512,activation='relu')(x1) #dense layer 3\n",
        "preds=Dense(2,activation='softmax')(x1) #final layer with softmax activation\n",
        "\n",
        "# x1=base_model.output\n",
        "# x1=GlobalAveragePooling2D()(x1)\n",
        "# x1=Dense(1024,activation='relu')(x1) #we add dense layers so that the model can learn more complex1 functions and classify for better results.\n",
        "# x1=Dense(1024,activation='relu')(x1) #dense layer 2\n",
        "# x1=Dense(512,activation='relu')(x1) #dense layer 3\n",
        "# preds=Dense(2,activation='softmax')(x1) #final layer with softmax activation\n",
        "\n",
        "model=Model(inputs=base_model.input,outputs=preds)\n",
        "#specify the inputs\n",
        "#specify the outputs\n",
        "#now a model has been created based on our architecture\n",
        "\n",
        "for layer in model.layers[:20]:\n",
        "    layer.trainable=False\n",
        "for layer in model.layers[20:]:\n",
        "    layer.trainable=True\n",
        "\n",
        "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
        "\n",
        "train_generator=train_datagen.flow_from_directory('/gdrive/My Drive/COVID Dataset/Train/', # this is where you specify the path to the main data folder\n",
        "                                                 target_size=(224,224),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 shuffle=True)\n",
        "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "# Adam optimizer\n",
        "# loss function will be categorical cross entropy\n",
        "# evaluation metric will be accuracy\n",
        "\n",
        "step_size_train=train_generator.n//train_generator.batch_size\n",
        "history=model.fit_generator(generator=train_generator,\n",
        "                   steps_per_epoch=step_size_train,\n",
        "                   epochs=10,verbose=1)\n",
        "test_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
        "\n",
        "test_generator=train_datagen.flow_from_directory('/gdrive/My Drive/COVID Dataset/Test/', # this is where you specify the path to the main data folder\n",
        "                                                 target_size=(224,224),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 shuffle=True)\n",
        "scores=model.evaluate_generator(generator=test_generator,verbose=1)\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "Y_pred = model.predict_generator(test_generator)\n",
        "y_pred_InceptionV3 = np.argmax(Y_pred,axis=1)\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(test_generator.classes, y_pred_InceptionV3))\n",
        "print('Classification Report')\n",
        "target_names = ['Normal', 'Positive']\n",
        "print(classification_report(test_generator.classes, y_pred_InceptionV3, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.5/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 8s 0us/step\n",
            "Found 348 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "10/10 [==============================] - 38s 4s/step - loss: 0.7775 - accuracy: 0.7911\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.0547 - accuracy: 0.9810\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 14s 1s/step - loss: 0.0144 - accuracy: 0.9937\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 13s 1s/step - loss: 0.0036 - accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 13s 1s/step - loss: 0.0263 - accuracy: 0.9936\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 13s 1s/step - loss: 0.0161 - accuracy: 0.9968\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 14s 1s/step - loss: 0.0235 - accuracy: 0.9937\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.0623 - accuracy: 0.9873\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 14s 1s/step - loss: 0.1289 - accuracy: 0.9563\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 13s 1s/step - loss: 0.0224 - accuracy: 0.9968\n",
            "Found 59 images belonging to 2 classes.\n",
            "2/2 [==============================] - 6s 3s/step\n",
            "Confusion Matrix\n",
            "[[ 5 12]\n",
            " [12 30]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.29      0.29      0.29        17\n",
            "    Positive       0.71      0.71      0.71        42\n",
            "\n",
            "    accuracy                           0.59        59\n",
            "   macro avg       0.50      0.50      0.50        59\n",
            "weighted avg       0.59      0.59      0.59        59\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqfVcLA4UIlj",
        "colab_type": "text"
      },
      "source": [
        "# MobileNetV2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvy2-PyrTmP1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 731
        },
        "outputId": "6a9e8a78-3da7-4415-9f0c-04d4d1fac9b1"
      },
      "source": [
        "base_model=MobileNetV2(weights='imagenet',include_top=False) #imports the mobilenet model and discards the last 1000 neuron layer.\n",
        "x1=base_model.output\n",
        "x1=GlobalAveragePooling2D(pool_size=(4,4))(x1)\n",
        "x1=Flatten(name=\"flatten\")(x1)\n",
        "x1=Dense(64,activation='relu')(x1) #we add dense layers so that the model can learn more complex1 functions and classify for better results.\n",
        "x1=Dropout(0.5)(x1)\n",
        "#x1=Dense(1024,activation='relu')(x1) #dense layer 2\n",
        "#x1=Dense(512,activation='relu')(x1) #dense layer 3\n",
        "preds=Dense(2,activation='softmax')(x1) #final layer with softmax activation\n",
        "\n",
        "# x1=base_model.output\n",
        "# x1=GlobalAveragePooling2D()(x1)\n",
        "# x1=Dense(1024,activation='relu')(x1) #we add dense layers so that the model can learn more complex1 functions and classify for better results.\n",
        "# x1=Dense(1024,activation='relu')(x1) #dense layer 2\n",
        "# x1=Dense(512,activation='relu')(x1) #dense layer 3\n",
        "# preds=Dense(2,activation='softmax')(x1) #final layer with softmax activation\n",
        "\n",
        "model=Model(inputs=base_model.input,outputs=preds)\n",
        "#specify the inputs\n",
        "#specify the outputs\n",
        "#now a model has been created based on our architecture\n",
        "\n",
        "for layer in model.layers[:20]:\n",
        "    layer.trainable=False\n",
        "for layer in model.layers[20:]:\n",
        "    layer.trainable=True\n",
        "\n",
        "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
        "\n",
        "train_generator=train_datagen.flow_from_directory('/gdrive/My Drive/COVID Dataset/Train/', # this is where you specify the path to the main data folder\n",
        "                                                 target_size=(224,224),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 shuffle=True)\n",
        "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "# Adam optimizer\n",
        "# loss function will be categorical cross entropy\n",
        "# evaluation metric will be accuracy\n",
        "\n",
        "step_size_train=train_generator.n//train_generator.batch_size\n",
        "history=model.fit_generator(generator=train_generator,\n",
        "                   steps_per_epoch=step_size_train,\n",
        "                   epochs=10,verbose=1)\n",
        "test_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
        "\n",
        "test_generator=train_datagen.flow_from_directory('/gdrive/My Drive/COVID Dataset/Test/', # this is where you specify the path to the main data folder\n",
        "                                                 target_size=(224,224),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 shuffle=True)\n",
        "scores=model.evaluate_generator(generator=test_generator,verbose=1)\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "Y_pred = model.predict_generator(test_generator)\n",
        "y_pred_MobileNetV2 = np.argmax(Y_pred,axis=1)\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(test_generator.classes, y_pred_MobileNetV2))\n",
        "print('Classification Report')\n",
        "target_names = ['Normal', 'Positive']\n",
        "print(classification_report(test_generator.classes, y_pred_MobileNetV2, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras_applications/mobilenet_v2.py:294: UserWarning: `input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n",
            "  warnings.warn('`input_shape` is undefined or non-square, '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/JonathanCMitchell/mobilenet_v2_keras/releases/download/v1.1/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9412608/9406464 [==============================] - 2s 0us/step\n",
            "Found 348 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "10/10 [==============================] - 25s 3s/step - loss: 0.2728 - accuracy: 0.8323\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 11s 1s/step - loss: 0.0450 - accuracy: 0.9968\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 13s 1s/step - loss: 0.0468 - accuracy: 0.9873\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.0415 - accuracy: 0.9844\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.0037 - accuracy: 0.9968\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.0052 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.0215 - accuracy: 0.9937\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.0459 - accuracy: 0.9937\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.0421 - accuracy: 0.9937\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 13s 1s/step - loss: 0.0114 - accuracy: 0.9968\n",
            "Found 59 images belonging to 2 classes.\n",
            "2/2 [==============================] - 4s 2s/step\n",
            "Confusion Matrix\n",
            "[[ 6 11]\n",
            " [14 28]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.30      0.35      0.32        17\n",
            "    Positive       0.72      0.67      0.69        42\n",
            "\n",
            "    accuracy                           0.58        59\n",
            "   macro avg       0.51      0.51      0.51        59\n",
            "weighted avg       0.60      0.58      0.59        59\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTD8PfOBUPpR",
        "colab_type": "text"
      },
      "source": [
        "# NASNetLarge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNFSWJzfTxW4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "outputId": "f3369d1a-fbda-49bc-e08c-22e6ca5b5e21"
      },
      "source": [
        "base_model=NASNetLarge(weights='imagenet',include_top=False) #imports the mobilenet model and discards the last 1000 neuron layer.\n",
        "x1=base_model.output\n",
        "x1=GlobalAveragePooling2D(pool_size=(4,4))(x1)\n",
        "x1=Flatten(name=\"flatten\")(x1)\n",
        "x1=Dense(64,activation='relu')(x1) #we add dense layers so that the model can learn more complex1 functions and classify for better results.\n",
        "x1=Dropout(0.5)(x1)\n",
        "#x1=Dense(1024,activation='relu')(x1) #dense layer 2\n",
        "#x1=Dense(512,activation='relu')(x1) #dense layer 3\n",
        "preds=Dense(2,activation='softmax')(x1) #final layer with softmax activation\n",
        "\n",
        "# x1=base_model.output\n",
        "# x1=GlobalAveragePooling2D()(x1)\n",
        "# x1=Dense(1024,activation='relu')(x1) #we add dense layers so that the model can learn more complex1 functions and classify for better results.\n",
        "# x1=Dense(1024,activation='relu')(x1) #dense layer 2\n",
        "# x1=Dense(512,activation='relu')(x1) #dense layer 3\n",
        "# preds=Dense(2,activation='softmax')(x1) #final layer with softmax activation\n",
        "\n",
        "model=Model(inputs=base_model.input,outputs=preds)\n",
        "#specify the inputs\n",
        "#specify the outputs\n",
        "#now a model has been created based on our architecture\n",
        "\n",
        "for layer in model.layers[:20]:\n",
        "    layer.trainable=False\n",
        "for layer in model.layers[20:]:\n",
        "    layer.trainable=True\n",
        "\n",
        "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
        "\n",
        "train_generator=train_datagen.flow_from_directory('/gdrive/My Drive/COVID Dataset/Train/', # this is where you specify the path to the main data folder\n",
        "                                                 target_size=(331, 331),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 shuffle=True)\n",
        "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "# Adam optimizer\n",
        "# loss function will be categorical cross entropy\n",
        "# evaluation metric will be accuracy\n",
        "\n",
        "step_size_train=train_generator.n//train_generator.batch_size\n",
        "history=model.fit_generator(generator=train_generator,\n",
        "                   steps_per_epoch=step_size_train,\n",
        "                   epochs=10,verbose=1)\n",
        "test_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
        "\n",
        "test_generator=train_datagen.flow_from_directory('/gdrive/My Drive/COVID Dataset/Test/', # this is where you specify the path to the main data folder\n",
        "                                                 target_size=(331, 331),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 shuffle=True)\n",
        "scores=model.evaluate_generator(generator=test_generator,verbose=1)\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "Y_pred = model.predict_generator(test_generator)\n",
        "y_pred_NASNetLarge = np.argmax(Y_pred,axis=1)\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(test_generator.classes, y_pred_NASNetLarge))\n",
        "print('Classification Report')\n",
        "target_names = ['Normal', 'Positive']\n",
        "print(classification_report(test_generator.classes, y_pred_NASNetLarge, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 348 images belonging to 2 classes.\n",
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-5c3e5fd73dbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m history=model.fit_generator(generator=train_generator,\n\u001b[1;32m     34\u001b[0m                    \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_size_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                    epochs=10,verbose=1)\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mtest_datagen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessing_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#included in our dependencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[32,1008,42,42] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node normal_concat_0_1/concat (defined at /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3009) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_keras_scratch_graph_856696]\n\nFunction call stack:\nkeras_scratch_graph\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCIJvMawVK-1",
        "colab_type": "text"
      },
      "source": [
        "# NASNetMobile"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYK03pnkUWf9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "outputId": "c8e25d83-1b4f-43d4-938d-11bc795cba5f"
      },
      "source": [
        "base_model=NASNetMobile(weights='imagenet',include_top=False) #imports the mobilenet model and discards the last 1000 neuron layer.\n",
        "x1=base_model.output\n",
        "x1=GlobalAveragePooling2D(pool_size=(4,4))(x1)\n",
        "x1=Flatten(name=\"flatten\")(x1)\n",
        "x1=Dense(64,activation='relu')(x1) #we add dense layers so that the model can learn more complex1 functions and classify for better results.\n",
        "x1=Dropout(0.5)(x1)\n",
        "#x1=Dense(1024,activation='relu')(x1) #dense layer 2\n",
        "#x1=Dense(512,activation='relu')(x1) #dense layer 3\n",
        "preds=Dense(2,activation='softmax')(x1) #final layer with softmax activation\n",
        "\n",
        "# x1=base_model.output\n",
        "# x1=GlobalAveragePooling2D()(x1)\n",
        "# x1=Dense(1024,activation='relu')(x1) #we add dense layers so that the model can learn more complex1 functions and classify for better results.\n",
        "# x1=Dense(1024,activation='relu')(x1) #dense layer 2\n",
        "# x1=Dense(512,activation='relu')(x1) #dense layer 3\n",
        "# preds=Dense(2,activation='softmax')(x1) #final layer with softmax activation\n",
        "\n",
        "model=Model(inputs=base_model.input,outputs=preds)\n",
        "#specify the inputs\n",
        "#specify the outputs\n",
        "#now a model has been created based on our architecture\n",
        "\n",
        "for layer in model.layers[:20]:\n",
        "    layer.trainable=False\n",
        "for layer in model.layers[20:]:\n",
        "    layer.trainable=True\n",
        "\n",
        "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
        "\n",
        "train_generator=train_datagen.flow_from_directory('/gdrive/My Drive/COVID Dataset/Train/', # this is where you specify the path to the main data folder\n",
        "                                                 target_size=(224,224),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 shuffle=True)\n",
        "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "# Adam optimizer\n",
        "# loss function will be categorical cross entropy\n",
        "# evaluation metric will be accuracy\n",
        "\n",
        "step_size_train=train_generator.n//train_generator.batch_size\n",
        "history=model.fit_generator(generator=train_generator,\n",
        "                   steps_per_epoch=step_size_train,\n",
        "                   epochs=10,verbose=1)\n",
        "test_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
        "\n",
        "test_generator=train_datagen.flow_from_directory('/gdrive/My Drive/COVID Dataset/Test/', # this is where you specify the path to the main data folder\n",
        "                                                 target_size=(224,224),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 shuffle=True)\n",
        "scores=model.evaluate_generator(generator=test_generator,verbose=1)\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "Y_pred = model.predict_generator(test_generator)\n",
        "y_pred_NASNetMobile = np.argmax(Y_pred,axis=1)\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(test_generator.classes, y_pred_NASNetMobile))\n",
        "print('Classification Report')\n",
        "target_names = ['Normal', 'Positive']\n",
        "print(classification_report(test_generator.classes, y_pred_NASNetMobile, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/titu1994/Keras-NASNet/releases/download/v1.2/NASNet-mobile-no-top.h5\n",
            "19996672/19993432 [==============================] - 2s 0us/step\n",
            "Found 348 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "10/10 [==============================] - 124s 12s/step - loss: 0.4668 - accuracy: 0.7785\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 14s 1s/step - loss: 0.0064 - accuracy: 0.9968\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 14s 1s/step - loss: 0.1307 - accuracy: 0.9842\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 14s 1s/step - loss: 0.0298 - accuracy: 0.9875\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 13s 1s/step - loss: 0.0129 - accuracy: 0.9968\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 14s 1s/step - loss: 4.9640e-04 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 14s 1s/step - loss: 1.7771e-06 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 14s 1s/step - loss: 0.0366 - accuracy: 0.9937\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 14s 1s/step - loss: 0.0763 - accuracy: 0.9937\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 14s 1s/step - loss: 0.0279 - accuracy: 0.9968\n",
            "Found 59 images belonging to 2 classes.\n",
            "2/2 [==============================] - 9s 5s/step\n",
            "Confusion Matrix\n",
            "[[17  0]\n",
            " [41  1]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.29      1.00      0.45        17\n",
            "    Positive       1.00      0.02      0.05        42\n",
            "\n",
            "    accuracy                           0.31        59\n",
            "   macro avg       0.65      0.51      0.25        59\n",
            "weighted avg       0.80      0.31      0.16        59\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ki6sXQlRVTvJ",
        "colab_type": "text"
      },
      "source": [
        "# ResNet101"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H84mFJksVOC0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "outputId": "816eacc7-b69b-4fe3-f81d-e97e16bbc346"
      },
      "source": [
        "base_model=ResNet101(weights='imagenet',include_top=False) #imports the mobilenet model and discards the last 1000 neuron layer.\n",
        "x1=base_model.output\n",
        "x1=GlobalAveragePooling2D(pool_size=(4,4))(x1)\n",
        "x1=Flatten(name=\"flatten\")(x1)\n",
        "x1=Dense(64,activation='relu')(x1) #we add dense layers so that the model can learn more complex1 functions and classify for better results.\n",
        "x1=Dropout(0.5)(x1)\n",
        "#x1=Dense(1024,activation='relu')(x1) #dense layer 2\n",
        "#x1=Dense(512,activation='relu')(x1) #dense layer 3\n",
        "preds=Dense(2,activation='softmax')(x1) #final layer with softmax activation\n",
        "\n",
        "# x1=base_model.output\n",
        "# x1=GlobalAveragePooling2D()(x1)\n",
        "# x1=Dense(1024,activation='relu')(x1) #we add dense layers so that the model can learn more complex1 functions and classify for better results.\n",
        "# x1=Dense(1024,activation='relu')(x1) #dense layer 2\n",
        "# x1=Dense(512,activation='relu')(x1) #dense layer 3\n",
        "# preds=Dense(2,activation='softmax')(x1) #final layer with softmax activation\n",
        "\n",
        "model=Model(inputs=base_model.input,outputs=preds)\n",
        "#specify the inputs\n",
        "#specify the outputs\n",
        "#now a model has been created based on our architecture\n",
        "\n",
        "for layer in model.layers[:20]:\n",
        "    layer.trainable=False\n",
        "for layer in model.layers[20:]:\n",
        "    layer.trainable=True\n",
        "\n",
        "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
        "\n",
        "train_generator=train_datagen.flow_from_directory('/gdrive/My Drive/COVID Dataset/Train/', # this is where you specify the path to the main data folder\n",
        "                                                 target_size=(224,224),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 shuffle=True)\n",
        "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "# Adam optimizer\n",
        "# loss function will be categorical cross entropy\n",
        "# evaluation metric will be accuracy\n",
        "\n",
        "step_size_train=train_generator.n//train_generator.batch_size\n",
        "history=model.fit_generator(generator=train_generator,\n",
        "                   steps_per_epoch=step_size_train,\n",
        "                   epochs=10,verbose=1)\n",
        "test_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
        "\n",
        "test_generator=train_datagen.flow_from_directory('/gdrive/My Drive/COVID Dataset/Test/', # this is where you specify the path to the main data folder\n",
        "                                                 target_size=(224,224),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 shuffle=True)\n",
        "scores=model.evaluate_generator(generator=test_generator,verbose=1)\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "Y_pred = model.predict_generator(test_generator)\n",
        "y_pred_ResNet101 = np.argmax(Y_pred,axis=1)\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(test_generator.classes, y_pred_ResNet101))\n",
        "print('Classification Report')\n",
        "target_names = ['Normal', 'Positive']\n",
        "print(classification_report(test_generator.classes, y_pred_ResNet101, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 348 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "10/10 [==============================] - 69s 7s/step - loss: 0.2630 - accuracy: 0.9114\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 21s 2s/step - loss: 0.5377 - accuracy: 0.9594\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 20s 2s/step - loss: 0.0313 - accuracy: 0.9936\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 20s 2s/step - loss: 0.2732 - accuracy: 0.9937\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 20s 2s/step - loss: 0.0405 - accuracy: 0.9905\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 20s 2s/step - loss: 0.0463 - accuracy: 0.9936\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 20s 2s/step - loss: 0.0286 - accuracy: 0.9905\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 20s 2s/step - loss: 0.0319 - accuracy: 0.9937\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 20s 2s/step - loss: 0.0139 - accuracy: 0.9968\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 20s 2s/step - loss: 0.0023 - accuracy: 1.0000\n",
            "Found 59 images belonging to 2 classes.\n",
            "2/2 [==============================] - 7s 3s/step\n",
            "Confusion Matrix\n",
            "[[ 0 17]\n",
            " [ 0 42]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.00      0.00      0.00        17\n",
            "    Positive       0.71      1.00      0.83        42\n",
            "\n",
            "    accuracy                           0.71        59\n",
            "   macro avg       0.36      0.50      0.42        59\n",
            "weighted avg       0.51      0.71      0.59        59\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMQ-IZltVVwX",
        "colab_type": "text"
      },
      "source": [
        "# ResNet152V2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YDJWUfKFVW1J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "outputId": "9d3e81d0-8b3b-4096-91af-5e52a9340534"
      },
      "source": [
        "base_model=ResNet152V2(weights='imagenet',include_top=False) #imports the mobilenet model and discards the last 1000 neuron layer.\n",
        "x1=base_model.output\n",
        "x1=GlobalAveragePooling2D(pool_size=(4,4))(x1)\n",
        "x1=Flatten(name=\"flatten\")(x1)\n",
        "x1=Dense(64,activation='relu')(x1) #we add dense layers so that the model can learn more complex1 functions and classify for better results.\n",
        "x1=Dropout(0.5)(x1)\n",
        "#x1=Dense(1024,activation='relu')(x1) #dense layer 2\n",
        "#x1=Dense(512,activation='relu')(x1) #dense layer 3\n",
        "preds=Dense(2,activation='softmax')(x1) #final layer with softmax activation\n",
        "\n",
        "# x1=base_model.output\n",
        "# x1=GlobalAveragePooling2D()(x1)\n",
        "# x1=Dense(1024,activation='relu')(x1) #we add dense layers so that the model can learn more complex1 functions and classify for better results.\n",
        "# x1=Dense(1024,activation='relu')(x1) #dense layer 2\n",
        "# x1=Dense(512,activation='relu')(x1) #dense layer 3\n",
        "# preds=Dense(2,activation='softmax')(x1) #final layer with softmax activation\n",
        "\n",
        "model=Model(inputs=base_model.input,outputs=preds)\n",
        "#specify the inputs\n",
        "#specify the outputs\n",
        "#now a model has been created based on our architecture\n",
        "\n",
        "for layer in model.layers[:20]:\n",
        "    layer.trainable=False\n",
        "for layer in model.layers[20:]:\n",
        "    layer.trainable=True\n",
        "\n",
        "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
        "\n",
        "train_generator=train_datagen.flow_from_directory('/gdrive/My Drive/COVID Dataset/Train/', # this is where you specify the path to the main data folder\n",
        "                                                 target_size=(224,224),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 shuffle=True)\n",
        "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "# Adam optimizer\n",
        "# loss function will be categorical cross entropy\n",
        "# evaluation metric will be accuracy\n",
        "\n",
        "step_size_train=train_generator.n//train_generator.batch_size\n",
        "history=model.fit_generator(generator=train_generator,\n",
        "                   steps_per_epoch=step_size_train,\n",
        "                   epochs=10,verbose=1)\n",
        "test_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
        "\n",
        "test_generator=train_datagen.flow_from_directory('/gdrive/My Drive/COVID Dataset/Test/', # this is where you specify the path to the main data folder\n",
        "                                                 target_size=(224,224),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 shuffle=True)\n",
        "scores=model.evaluate_generator(generator=test_generator,verbose=1)\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "Y_pred = model.predict_generator(test_generator)\n",
        "y_pred_ResNet152V2 = np.argmax(Y_pred,axis=1)\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(test_generator.classes, y_pred_ResNet152V2))\n",
        "print('Classification Report')\n",
        "target_names = ['Normal', 'Positive']\n",
        "print(classification_report(test_generator.classes, y_pred_ResNet152V2, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/keras-team/keras-applications/releases/download/resnet/resnet152v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "234553344/234545216 [==============================] - 16s 0us/step\n",
            "Found 348 images belonging to 2 classes.\n",
            "Epoch 1/10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-2be88d8ec8fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m history=model.fit_generator(generator=train_generator,\n\u001b[1;32m     34\u001b[0m                    \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep_size_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                    epochs=10,verbose=1)\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mtest_datagen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mImageDataGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessing_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocess_input\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#included in our dependencies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3790\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3791\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3792\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3794\u001b[0m     \u001b[0;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1604\u001b[0m     \"\"\"\n\u001b[0;32m-> 1605\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1643\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m   1644\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m-> 1645\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[1024,256,1,1] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node conv4_block34_3_conv/convolution (defined at /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3009) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_keras_scratch_graph_1235938]\n\nFunction call stack:\nkeras_scratch_graph\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TBn-mX58VgSI",
        "colab_type": "text"
      },
      "source": [
        "# VGG16"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWREo_QrVjk8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "outputId": "f353eaac-a890-4219-96ce-d3777c032c21"
      },
      "source": [
        "base_model=VGG16(weights='imagenet',include_top=False) #imports the mobilenet model and discards the last 1000 neuron layer.\n",
        "x1=base_model.output\n",
        "x1=GlobalAveragePooling2D(pool_size=(4,4))(x1)\n",
        "x1=Flatten(name=\"flatten\")(x1)\n",
        "x1=Dense(64,activation='relu')(x1) #we add dense layers so that the model can learn more complex1 functions and classify for better results.\n",
        "x1=Dropout(0.5)(x1)\n",
        "#x1=Dense(1024,activation='relu')(x1) #dense layer 2\n",
        "#x1=Dense(512,activation='relu')(x1) #dense layer 3\n",
        "preds=Dense(2,activation='softmax')(x1) #final layer with softmax activation\n",
        "\n",
        "# x1=base_model.output\n",
        "# x1=GlobalAveragePooling2D()(x1)\n",
        "# x1=Dense(1024,activation='relu')(x1) #we add dense layers so that the model can learn more complex1 functions and classify for better results.\n",
        "# x1=Dense(1024,activation='relu')(x1) #dense layer 2\n",
        "# x1=Dense(512,activation='relu')(x1) #dense layer 3\n",
        "# preds=Dense(2,activation='softmax')(x1) #final layer with softmax activation\n",
        "\n",
        "model=Model(inputs=base_model.input,outputs=preds)\n",
        "#specify the inputs\n",
        "#specify the outputs\n",
        "#now a model has been created based on our architecture\n",
        "\n",
        "for layer in model.layers[:20]:\n",
        "    layer.trainable=False\n",
        "for layer in model.layers[20:]:\n",
        "    layer.trainable=True\n",
        "\n",
        "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
        "\n",
        "train_generator=train_datagen.flow_from_directory('/gdrive/My Drive/COVID Dataset/Train/', # this is where you specify the path to the main data folder\n",
        "                                                 target_size=(224,224),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 shuffle=True)\n",
        "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "# Adam optimizer\n",
        "# loss function will be categorical cross entropy\n",
        "# evaluation metric will be accuracy\n",
        "\n",
        "step_size_train=train_generator.n//train_generator.batch_size\n",
        "history=model.fit_generator(generator=train_generator,\n",
        "                   steps_per_epoch=step_size_train,\n",
        "                   epochs=10,verbose=1)\n",
        "test_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
        "\n",
        "test_generator=train_datagen.flow_from_directory('/gdrive/My Drive/COVID Dataset/Test/', # this is where you specify the path to the main data folder\n",
        "                                                 target_size=(224,224),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 shuffle=True)\n",
        "scores=model.evaluate_generator(generator=test_generator,verbose=1)\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "Y_pred = model.predict_generator(test_generator)\n",
        "y_pred_VGG16 = np.argmax(Y_pred,axis=1)\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(test_generator.classes, y_pred_VGG16))\n",
        "print('Classification Report')\n",
        "target_names = ['Normal', 'Positive']\n",
        "print(classification_report(test_generator.classes, y_pred_VGG16, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58892288/58889256 [==============================] - 5s 0us/step\n",
            "Found 348 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "10/10 [==============================] - 13s 1s/step - loss: 0.6598 - accuracy: 0.6344\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.1496 - accuracy: 0.9715\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 11s 1s/step - loss: 0.0118 - accuracy: 1.0000\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.0095 - accuracy: 0.9936\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 11s 1s/step - loss: 0.0034 - accuracy: 0.9968\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 11s 1s/step - loss: 0.0015 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.0011 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 11s 1s/step - loss: 1.2495e-04 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 11s 1s/step - loss: 2.0681e-04 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 12s 1s/step - loss: 2.9758e-05 - accuracy: 1.0000\n",
            "Found 59 images belonging to 2 classes.\n",
            "2/2 [==============================] - 5s 3s/step\n",
            "Confusion Matrix\n",
            "[[ 6 11]\n",
            " [11 31]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.35      0.35      0.35        17\n",
            "    Positive       0.74      0.74      0.74        42\n",
            "\n",
            "    accuracy                           0.63        59\n",
            "   macro avg       0.55      0.55      0.55        59\n",
            "weighted avg       0.63      0.63      0.63        59\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FRJKbTuVrGJ",
        "colab_type": "text"
      },
      "source": [
        "# VGG19"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLb0GDQMVsU2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "outputId": "52420a42-c473-4a6f-bb25-65b5c7fda02a"
      },
      "source": [
        "base_model=VGG19(weights='imagenet',include_top=False) #imports the mobilenet model and discards the last 1000 neuron layer.\n",
        "x1=base_model.output\n",
        "x1=GlobalAveragePooling2D(pool_size=(4,4))(x1)\n",
        "x1=Flatten(name=\"flatten\")(x1)\n",
        "x1=Dense(64,activation='relu')(x1) #we add dense layers so that the model can learn more complex1 functions and classify for better results.\n",
        "x1=Dropout(0.5)(x1)\n",
        "#x1=Dense(1024,activation='relu')(x1) #dense layer 2\n",
        "#x1=Dense(512,activation='relu')(x1) #dense layer 3\n",
        "preds=Dense(2,activation='softmax')(x1) #final layer with softmax activation\n",
        "\n",
        "# x1=base_model.output\n",
        "# x1=GlobalAveragePooling2D()(x1)\n",
        "# x1=Dense(1024,activation='relu')(x1) #we add dense layers so that the model can learn more complex1 functions and classify for better results.\n",
        "# x1=Dense(1024,activation='relu')(x1) #dense layer 2\n",
        "# x1=Dense(512,activation='relu')(x1) #dense layer 3\n",
        "# preds=Dense(2,activation='softmax')(x1) #final layer with softmax activation\n",
        "\n",
        "model=Model(inputs=base_model.input,outputs=preds)\n",
        "#specify the inputs\n",
        "#specify the outputs\n",
        "#now a model has been created based on our architecture\n",
        "\n",
        "for layer in model.layers[:20]:\n",
        "    layer.trainable=False\n",
        "for layer in model.layers[20:]:\n",
        "    layer.trainable=True\n",
        "\n",
        "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
        "\n",
        "train_generator=train_datagen.flow_from_directory('/gdrive/My Drive/COVID Dataset/Train/', # this is where you specify the path to the main data folder\n",
        "                                                 target_size=(224,224),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 shuffle=True)\n",
        "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "# Adam optimizer\n",
        "# loss function will be categorical cross entropy\n",
        "# evaluation metric will be accuracy\n",
        "\n",
        "step_size_train=train_generator.n//train_generator.batch_size\n",
        "history=model.fit_generator(generator=train_generator,\n",
        "                   steps_per_epoch=step_size_train,\n",
        "                   epochs=10,verbose=1)\n",
        "test_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
        "\n",
        "test_generator=train_datagen.flow_from_directory('/gdrive/My Drive/COVID Dataset/Test/', # this is where you specify the path to the main data folder\n",
        "                                                 target_size=(224,224),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 shuffle=True)\n",
        "scores=model.evaluate_generator(generator=test_generator,verbose=1)\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "Y_pred = model.predict_generator(test_generator)\n",
        "y_pred_VGG19 = np.argmax(Y_pred,axis=1)\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(test_generator.classes, y_pred_VGG19))\n",
        "print('Classification Report')\n",
        "target_names = ['Normal', 'Positive']\n",
        "print(classification_report(test_generator.classes, y_pred_VGG19, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 8s 0us/step\n",
            "Found 348 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.5753 - accuracy: 0.7469\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.0779 - accuracy: 0.9652\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.0869 - accuracy: 0.9712\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.0129 - accuracy: 0.9968\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 11s 1s/step - loss: 0.0067 - accuracy: 0.9968\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 12s 1s/step - loss: 0.0017 - accuracy: 1.0000\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 12s 1s/step - loss: 2.0238e-05 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 12s 1s/step - loss: 5.9193e-05 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 12s 1s/step - loss: 2.6621e-05 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 11s 1s/step - loss: 5.0452e-06 - accuracy: 1.0000\n",
            "Found 59 images belonging to 2 classes.\n",
            "2/2 [==============================] - 3s 2s/step\n",
            "Confusion Matrix\n",
            "[[ 3 14]\n",
            " [14 28]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.18      0.18      0.18        17\n",
            "    Positive       0.67      0.67      0.67        42\n",
            "\n",
            "    accuracy                           0.53        59\n",
            "   macro avg       0.42      0.42      0.42        59\n",
            "weighted avg       0.53      0.53      0.53        59\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-WGA7hO4Vw9k",
        "colab_type": "text"
      },
      "source": [
        "# Xception"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnJeNQWqVyyx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "outputId": "b4992090-3fa6-469c-ec36-9cb010869737"
      },
      "source": [
        "base_model=Xception(weights='imagenet',include_top=False) #imports the mobilenet model and discards the last 1000 neuron layer.\n",
        "x1=base_model.output\n",
        "x1=GlobalAveragePooling2D(pool_size=(4,4))(x1)\n",
        "x1=Flatten(name=\"flatten\")(x1)\n",
        "x1=Dense(64,activation='relu')(x1) #we add dense layers so that the model can learn more complex1 functions and classify for better results.\n",
        "x1=Dropout(0.5)(x1)\n",
        "#x1=Dense(1024,activation='relu')(x1) #dense layer 2\n",
        "#x1=Dense(512,activation='relu')(x1) #dense layer 3\n",
        "preds=Dense(2,activation='softmax')(x1) #final layer with softmax activation\n",
        "\n",
        "# x1=base_model.output\n",
        "# x1=GlobalAveragePooling2D()(x1)\n",
        "# x1=Dense(1024,activation='relu')(x1) #we add dense layers so that the model can learn more complex1 functions and classify for better results.\n",
        "# x1=Dense(1024,activation='relu')(x1) #dense layer 2\n",
        "# x1=Dense(512,activation='relu')(x1) #dense layer 3\n",
        "# preds=Dense(2,activation='softmax')(x1) #final layer with softmax activation\n",
        "\n",
        "model=Model(inputs=base_model.input,outputs=preds)\n",
        "#specify the inputs\n",
        "#specify the outputs\n",
        "#now a model has been created based on our architecture\n",
        "\n",
        "for layer in model.layers[:20]:\n",
        "    layer.trainable=False\n",
        "for layer in model.layers[20:]:\n",
        "    layer.trainable=True\n",
        "\n",
        "train_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
        "\n",
        "train_generator=train_datagen.flow_from_directory('/gdrive/My Drive/COVID Dataset/Train/', # this is where you specify the path to the main data folder\n",
        "                                                 target_size=(224,224),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 shuffle=True)\n",
        "model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "# Adam optimizer\n",
        "# loss function will be categorical cross entropy\n",
        "# evaluation metric will be accuracy\n",
        "\n",
        "step_size_train=train_generator.n//train_generator.batch_size\n",
        "history=model.fit_generator(generator=train_generator,\n",
        "                   steps_per_epoch=step_size_train,\n",
        "                   epochs=10,verbose=1)\n",
        "test_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) #included in our dependencies\n",
        "\n",
        "test_generator=train_datagen.flow_from_directory('/gdrive/My Drive/COVID Dataset/Test/', # this is where you specify the path to the main data folder\n",
        "                                                 target_size=(224,224),\n",
        "                                                 color_mode='rgb',\n",
        "                                                 batch_size=32,\n",
        "                                                 class_mode='categorical',\n",
        "                                                 shuffle=True)\n",
        "scores=model.evaluate_generator(generator=test_generator,verbose=1)\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "Y_pred = model.predict_generator(test_generator)\n",
        "y_pred_Xception = np.argmax(Y_pred,axis=1)\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(test_generator.classes, y_pred_Xception))\n",
        "print('Classification Report')\n",
        "target_names = ['Normal', 'Positive']\n",
        "print(classification_report(test_generator.classes, y_pred_Xception, target_names=target_names))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 7s 0us/step\n",
            "Found 348 images belonging to 2 classes.\n",
            "Epoch 1/10\n",
            "10/10 [==============================] - 33s 3s/step - loss: 0.4412 - accuracy: 0.8133\n",
            "Epoch 2/10\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.0650 - accuracy: 0.9842\n",
            "Epoch 3/10\n",
            "10/10 [==============================] - 14s 1s/step - loss: 0.3588 - accuracy: 0.9778\n",
            "Epoch 4/10\n",
            "10/10 [==============================] - 16s 2s/step - loss: 0.0572 - accuracy: 1.0000\n",
            "Epoch 5/10\n",
            "10/10 [==============================] - 15s 2s/step - loss: 0.1199 - accuracy: 0.9905\n",
            "Epoch 6/10\n",
            "10/10 [==============================] - 15s 2s/step - loss: 0.0166 - accuracy: 0.9968\n",
            "Epoch 7/10\n",
            "10/10 [==============================] - 15s 2s/step - loss: 0.0167 - accuracy: 1.0000\n",
            "Epoch 8/10\n",
            "10/10 [==============================] - 15s 1s/step - loss: 0.0012 - accuracy: 1.0000\n",
            "Epoch 9/10\n",
            "10/10 [==============================] - 16s 2s/step - loss: 5.8082e-04 - accuracy: 1.0000\n",
            "Epoch 10/10\n",
            "10/10 [==============================] - 15s 1s/step - loss: 5.5349e-05 - accuracy: 1.0000\n",
            "Found 59 images belonging to 2 classes.\n",
            "2/2 [==============================] - 5s 2s/step\n",
            "Confusion Matrix\n",
            "[[ 5 12]\n",
            " [11 31]]\n",
            "Classification Report\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.31      0.29      0.30        17\n",
            "    Positive       0.72      0.74      0.73        42\n",
            "\n",
            "    accuracy                           0.61        59\n",
            "   macro avg       0.52      0.52      0.52        59\n",
            "weighted avg       0.60      0.61      0.61        59\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WTw-vPh8T4GG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "Y_pred=pd.DataFrame({\n",
        "    'DenseNet201':y_pred_DenseNet201,\n",
        "    'InceptionResNetV2':y_pred_InceptionResNetV2,\n",
        "    'InceptionV3':y_pred_InceptionV3,\n",
        "    'MobileNet':y_pred_MobileNet,\n",
        "    'MobileNetV2':y_pred_MobileNetV2,\n",
        "    'NASNetMobile':y_pred_NASNetMobile,\n",
        "    'ResNet101':y_pred_ResNet101,\n",
        "    'VGG16':y_pred_VGG16,\n",
        "    'VGG19':y_pred_VGG19,\n",
        "    'Xception':y_pred_Xception,\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rtOjZLMCh69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_pred.to_csv(\"Predictions.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qwfzFwVdDDU1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "outputId": "b6ac6c28-942d-42e2-c22f-22d69e965dfc"
      },
      "source": [
        "Y_pred.corr()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DenseNet201</th>\n",
              "      <th>InceptionResNetV2</th>\n",
              "      <th>InceptionV3</th>\n",
              "      <th>MobileNet</th>\n",
              "      <th>MobileNetV2</th>\n",
              "      <th>NASNetMobile</th>\n",
              "      <th>ResNet101</th>\n",
              "      <th>VGG16</th>\n",
              "      <th>VGG19</th>\n",
              "      <th>Xception</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>DenseNet201</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.015404</td>\n",
              "      <td>-0.015404</td>\n",
              "      <td>-0.091138</td>\n",
              "      <td>-0.179307</td>\n",
              "      <td>0.112594</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.091138</td>\n",
              "      <td>-0.242607</td>\n",
              "      <td>-0.137305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>InceptionResNetV2</th>\n",
              "      <td>-0.015404</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.074230</td>\n",
              "      <td>0.091036</td>\n",
              "      <td>0.097820</td>\n",
              "      <td>0.083538</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.008403</td>\n",
              "      <td>0.091036</td>\n",
              "      <td>0.032816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>InceptionV3</th>\n",
              "      <td>-0.015404</td>\n",
              "      <td>-0.074230</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.074230</td>\n",
              "      <td>0.097820</td>\n",
              "      <td>-0.206389</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.074230</td>\n",
              "      <td>0.173669</td>\n",
              "      <td>0.032816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MobileNet</th>\n",
              "      <td>-0.091138</td>\n",
              "      <td>0.091036</td>\n",
              "      <td>-0.074230</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.139360</td>\n",
              "      <td>0.083538</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.074230</td>\n",
              "      <td>0.173669</td>\n",
              "      <td>0.116996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>MobileNetV2</th>\n",
              "      <td>-0.179307</td>\n",
              "      <td>0.097820</td>\n",
              "      <td>0.097820</td>\n",
              "      <td>-0.139360</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.183359</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.097820</td>\n",
              "      <td>-0.060300</td>\n",
              "      <td>0.046413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>NASNetMobile</th>\n",
              "      <td>0.112594</td>\n",
              "      <td>0.083538</td>\n",
              "      <td>-0.206389</td>\n",
              "      <td>0.083538</td>\n",
              "      <td>-0.183359</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.083538</td>\n",
              "      <td>0.083538</td>\n",
              "      <td>0.080096</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ResNet101</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>VGG16</th>\n",
              "      <td>-0.091138</td>\n",
              "      <td>0.008403</td>\n",
              "      <td>-0.074230</td>\n",
              "      <td>-0.074230</td>\n",
              "      <td>0.097820</td>\n",
              "      <td>0.083538</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.256303</td>\n",
              "      <td>-0.051364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>VGG19</th>\n",
              "      <td>-0.242607</td>\n",
              "      <td>0.091036</td>\n",
              "      <td>0.173669</td>\n",
              "      <td>0.173669</td>\n",
              "      <td>-0.060300</td>\n",
              "      <td>0.083538</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.256303</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.219724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Xception</th>\n",
              "      <td>-0.137305</td>\n",
              "      <td>0.032816</td>\n",
              "      <td>0.032816</td>\n",
              "      <td>0.116996</td>\n",
              "      <td>0.046413</td>\n",
              "      <td>0.080096</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-0.051364</td>\n",
              "      <td>-0.219724</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   DenseNet201  InceptionResNetV2  ...     VGG19  Xception\n",
              "DenseNet201           1.000000          -0.015404  ... -0.242607 -0.137305\n",
              "InceptionResNetV2    -0.015404           1.000000  ...  0.091036  0.032816\n",
              "InceptionV3          -0.015404          -0.074230  ...  0.173669  0.032816\n",
              "MobileNet            -0.091138           0.091036  ...  0.173669  0.116996\n",
              "MobileNetV2          -0.179307           0.097820  ... -0.060300  0.046413\n",
              "NASNetMobile          0.112594           0.083538  ...  0.083538  0.080096\n",
              "ResNet101                  NaN                NaN  ...       NaN       NaN\n",
              "VGG16                -0.091138           0.008403  ...  0.256303 -0.051364\n",
              "VGG19                -0.242607           0.091036  ...  1.000000 -0.219724\n",
              "Xception             -0.137305           0.032816  ... -0.219724  1.000000\n",
              "\n",
              "[10 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sdilPbbGESX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Y_corr=Y_pred.corr()\n",
        "Y_corr.to_csv(\"Correlation.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CfjPb5X0IStf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}